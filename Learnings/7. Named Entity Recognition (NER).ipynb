{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0007c1-3fa9-42bf-b195-7379b628aeb4",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Named Entity Recognition (NER)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12362ee5-6062-4457-b568-bec0c885ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1862b3a6-1d02-4c2f-8eb9-fee21913a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8afc8ad-d81b-4ee4-a1fd-0265cf0e8e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tata Services  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Zudio  |  GPE  |  Countries, cities, states\n",
      "$25 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tata Services going to acquire Zudio for $25 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \",ent.label_,\" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d88f0a-f614-42d8-81c6-a1ebc3e0d6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tata Services\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " going to acquire \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zudio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $25 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9ebf89-5080-4a60-9a64-de3ab42693ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels[\"ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc23d9a2-b5c5-4a2e-a453-6ee4400130c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Michael Bloomberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " founded \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bloomberg Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1982\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Michael Bloomberg founded Bloomberg Inc in 1982\")\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660aca05-2c5c-494e-b310-3b338df3aa5e",
   "metadata": {},
   "source": [
    "<h3>Setting custom entities<h3></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61c2616-932a-4eb7-a107-0b5a19e7c4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "going to acquire"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concepts of span\n",
    "doc = nlp(\"Tata Services going to acquire Zudio for $25 billion\")\n",
    "\n",
    "doc [2:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4876a464-de7b-4ec9-ae62-2b964e0beac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc [2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fd1b605-b134-40ec-8ec8-1dcc2c736da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label=\"ORG\")\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60bab4b6-c7f0-42e7-9c8a-d2e5da5e8c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tata"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "513ee1c5-1e2f-4061-ae71-86f6ffaf7d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Zudio"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6cae39f-fa6c-4557-9fd3-f831e695d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.set_ents([s1, s2], default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df8d686f-9088-4c55-be59-953e391c283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tata Services  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Zudio  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$25 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \",ent.label_,\" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aebe3b-ffbb-462f-a7ca-efd016b0f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Own NER\n",
    "\n",
    "#We use entity ruler for this\n",
    "\n",
    "#https://spacy.io/api/entityruler/\n",
    "#https://python-textbook.pythonhumanities.com/03_spacy/03_02_02_entityruler.html#introducing-complex-rules-and-variance-to-the-entityruler-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ed4a9-8a7f-41a5-897d-2c1105a72a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#approch 3: machine Learning - CRF ( Condtional random Fields) - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d666778-3349-4b6e-83b4-1c3620c966ef",
   "metadata": {},
   "source": [
    "**Excersie: 1**\n",
    "\n",
    "* Extract all the Geographical (cities, Countries, states) names from a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd052286-ffdb-4294-8791-68bc3f47f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d83b7dc-e7a1-4c4c-a929-c316d1a4d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiran  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "India  |  GPE  |  Countries, cities, states\n",
      "Google  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Google  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Delhi  |  GPE  |  Countries, cities, states\n",
      "Chaat  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Gujarat  |  GPE  |  Countries, cities, states\n",
      "Dal Dhokli  |  PERSON  |  People, including fictional\n",
      "Tamilnadu  |  GPE  |  Countries, cities, states\n",
      "Pongal  |  GPE  |  Countries, cities, states\n",
      "Andhrapradesh  |  GPE  |  Countries, cities, states\n",
      "Biryani  |  PERSON  |  People, including fictional\n",
      "Assam  |  GPE  |  Countries, cities, states\n",
      "Papaya Khar  |  PERSON  |  People, including fictional\n",
      "Bihar  |  GPE  |  Countries, cities, states\n",
      "Litti Chowkha  |  PERSON  |  People, including fictional\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \",ent.label_,\" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0781bb24-b415-46f5-bd86-20053a9dea14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pongal"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0c7336f-78be-4b24-8732-e2c3dc774fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographical ares in given text:\n",
      "India, Delhi, Gujarat, Tamilnadu, Pongal, Andhrapradesh, Assam, Bihar\n"
     ]
    }
   ],
   "source": [
    "#s1 = Span(doc, 45, 46, label=\"ORG\")\n",
    "#I need to remove ponagla as a food,\n",
    "\n",
    "Geographical_areas= []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"GPE\":\n",
    "        Geographical_areas.append(ent)\n",
    "\n",
    "Geographical_areas = [str(area) for area in Geographical_areas]\n",
    "print(\"Geographical ares in given text:\")\n",
    "print(\", \".join(Geographical_areas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2f25a-5fb0-4614-b539-b64daf543d76",
   "metadata": {},
   "source": [
    "**Excersie: 2**\n",
    "* Extract all the birth dates of cricketers in the given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "502c1a2f-a442-4165-945f-0f8ba3284f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c2cdee7-a788-4c1f-ac5a-a0570fa2ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sachin Tendulkar  |  PERSON  |  People, including fictional\n",
      "24 April 1973  |  DATE  |  Absolute or relative dates or periods\n",
      "Virat Kholi  |  LOC  |  Non-GPE locations, mountain ranges, bodies of water\n",
      "5 November 1988  |  DATE  |  Absolute or relative dates or periods\n",
      "Dhoni  |  PERSON  |  People, including fictional\n",
      "7 July 1981  |  DATE  |  Absolute or relative dates or periods\n",
      "Ricky  |  PERSON  |  People, including fictional\n",
      "19 December 1974  |  DATE  |  Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \",ent.label_,\" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "301295f0-75dc-456b-a2ed-2fc98f89b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of Births in given text: [24 April 1973, 5 November 1988, 7 July 1981, 19 December 1974]\n",
      "Number of DOB in text 4\n"
     ]
    }
   ],
   "source": [
    "date_of_births= []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"DATE\":\n",
    "        date_of_births.append(ent)\n",
    "\n",
    "print(\"Date of Births in given text:\", date_of_births)\n",
    "print(\"Number of DOB in text\", len(date_of_births))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163300e-2dda-47d4-b28b-eca9418ee3f5",
   "metadata": {},
   "source": [
    "<h2>Creating Customized entity for Food</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5224fb22-63b2-48b2-a740-382ae6160c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "# Load a pre-trained spaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add named entity recognizer to the pipeline\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Define food types as a new entity label\n",
    "ner.add_label(\"FOOD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b165a1dc-92d5-41ff-ac5a-e9cf969fb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training data\n",
    "TRAIN_DATA = [\n",
    "    (\"I love pizza.\", {\"entities\": [(7, 12, \"FOOD\")]}),\n",
    "    (\"Sushi is delicious.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "    (\"Chaat.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "    (\"Dal Dhokli,\", {\"entities\": [(0, 10, \"FOOD\")]}),\n",
    "    (\"Pongal.\", {\"entities\": [(0, 6, \"FOOD\")]}),\n",
    "    (\"Biryani.\", {\"entities\": [(0, 7, \"FOOD\")]}),\n",
    "    (\"Papaya Khar.\", {\"entities\": [(0, 11, \"FOOD\")]}),\n",
    "    (\"Chowkha.\", {\"entities\": [(0, 7, \"FOOD\")]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8711bf65-3651-425b-89e5-938a17491884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I love pizza.', {'entities': [(7, 12, 'FOOD')]}),\n",
       " ('Sushi is delicious.', {'entities': [(0, 5, 'FOOD')]}),\n",
       " ('Chaat.', {'entities': [(0, 5, 'FOOD')]}),\n",
       " ('Dal Dhokli,', {'entities': [(0, 10, 'FOOD')]}),\n",
       " ('Pongal.', {'entities': [(0, 6, 'FOOD')]}),\n",
       " ('Biryani.', {'entities': [(0, 7, 'FOOD')]}),\n",
       " ('Papaya Khar.', {'entities': [(0, 11, 'FOOD')]}),\n",
       " ('Chowkha.', {'entities': [(0, 7, 'FOOD')]})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a8df83f-03a8-4bec-86f3-58847ab126ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E022] Could not find a transition with the name 'O' in the NER model.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mmake_doc(text)\n\u001b[0;32m      4\u001b[0m example \u001b[38;5;241m=\u001b[39m Example\u001b[38;5;241m.\u001b[39mfrom_dict(doc, annotations)\n\u001b[1;32m----> 5\u001b[0m nlp\u001b[38;5;241m.\u001b[39mupdate([example], drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, losses\u001b[38;5;241m=\u001b[39m{})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\language.py:1193\u001b[0m, in \u001b[0;36mLanguage.update\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, proc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline:\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;66;03m# ignore statements are used here because mypy ignores hasattr\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1193\u001b[0m         proc\u001b[38;5;241m.\u001b[39mupdate(examples, sgd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, losses\u001b[38;5;241m=\u001b[39mlosses, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg[name])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sgd \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1195\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1196\u001b[0m             name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude\n\u001b[0;32m   1197\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc, ty\u001b[38;5;241m.\u001b[39mTrainableComponent)\n\u001b[0;32m   1198\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mis_trainable\n\u001b[0;32m   1199\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1200\u001b[0m         ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:411\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:671\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser._init_gold_batch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\ner.pyx:297\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoPushDown.init_gold\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\ner.pyx:61\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoGold.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\ner.pyx:89\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.create_gold_state\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\ner.pyx:201\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoPushDown.lookup_transition\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[E022] Could not find a transition with the name 'O' in the NER model.\""
     ]
    }
   ],
   "source": [
    "# Train the NER model\n",
    "for text, annotations in TRAIN_DATA:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    nlp.update([example], drop=0.5, losses={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716868d-91ce-4021-8334-911a1fdafba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb69c4a6-6b90-4e74-ac47-e40ab979910c",
   "metadata": {},
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "# Load a pre-trained spaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add named entity recognizer to the pipeline\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Define food types as a new entity label\n",
    "ner.add_label(\"FOOD\")\n",
    "\n",
    "# Example training data with \"O\" labels for non-entity tokens\n",
    "TRAIN_DATA = [\n",
    "    (\"I love pizza.\", {\"entities\": [(7, 12, \"FOOD\")]}),\n",
    "    (\"Sushi is delicious.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "    # Add more examples with labeled food entities\n",
    "    (\"I want to visit Paris.\", {\"entities\": []}),  # No entities in this sentence\n",
    "]\n",
    "\n",
    "# Train the NER model\n",
    "for text, annotations in TRAIN_DATA:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    nlp.update([example], drop=0.5, losses={})\n",
    "\n",
    "# Test the trained model\n",
    "test_text = \"I ordered some sushi and a salad.\"\n",
    "doc = nlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d400c6f1-47eb-4bca-a631-d663441630a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "text =  \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6adbf121-d3da-4ca7-b556-d624c122d82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chaat"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "234d41b5-3f6c-416d-8ddf-47a14c28535a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dal Dhokli"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[38:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c7c66db-3405-40d4-a36d-31c5ed5ffa19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pongal"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75d69ef3-ec1d-422a-8c5c-c653986a37b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Biryani"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc79bab5-0ec3-432c-8632-79b66ff3cb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Papaya Khar"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[57:59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "53f21986-d1bd-4a40-bba0-ca5911e4c255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Litti Chowkha"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[65:67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7aab07be-78b1-4668-ba8d-d56c9f7635f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 32, 33, label=\"FOOD\")\n",
    "s2 = Span(doc, 38, 40, label=\"FOOD\")\n",
    "s3 = Span(doc, 45, 46, label=\"FOOD\")\n",
    "s4 = Span(doc, 51, 52, label=\"FOOD\")\n",
    "s5 = Span(doc, 57, 59, label=\"FOOD\")\n",
    "s6 = Span(doc, 65, 67, label=\"FOOD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51ac1773-e77c-4162-88d2-c3b1c912064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.set_ents([s1, s2, s3, s4, s5, s6], default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "07ec36ca-cb01-4336-98ac-a3c13272d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiran  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "India  |  GPE  |  Countries, cities, states\n",
      "Google  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Google  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Delhi  |  GPE  |  Countries, cities, states\n",
      "Chaat  |  FOOD  |  None\n",
      "Gujarat  |  GPE  |  Countries, cities, states\n",
      "Dal Dhokli  |  FOOD  |  None\n",
      "Tamilnadu  |  GPE  |  Countries, cities, states\n",
      "Pongal  |  FOOD  |  None\n",
      "Andhrapradesh  |  GPE  |  Countries, cities, states\n",
      "Biryani  |  FOOD  |  None\n",
      "Assam  |  GPE  |  Countries, cities, states\n",
      "Papaya Khar  |  FOOD  |  None\n",
      "Bihar  |  GPE  |  Countries, cities, states\n",
      "Litti Chowkha  |  FOOD  |  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term 'FOOD' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \",ent.label_,\" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb8a2d20-2a78-4b27-b8cd-ab011d2a89a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographical ares in given text:\n",
      "India, Delhi, Gujarat, Tamilnadu, Andhrapradesh, Assam, Bihar\n"
     ]
    }
   ],
   "source": [
    "Geographical_areas= []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"GPE\":\n",
    "        Geographical_areas.append(ent)\n",
    "\n",
    "Geographical_areas = [str(area) for area in Geographical_areas]\n",
    "print(\"Geographical ares in given text:\")\n",
    "print(\", \".join(Geographical_areas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63307da-1a9a-421c-bd7c-ced0cf41c3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f5a8983-ac95-409f-bbe6-51b0fad7012e",
   "metadata": {},
   "source": [
    "Customized Named entity\n",
    "\n",
    "https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d598ef-5e93-449e-9aae-57a6374df375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
